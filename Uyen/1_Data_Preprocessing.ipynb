{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing for Reddit Post Virality Prediction\n",
    "\n",
    "This notebook covers the data preprocessing pipeline for predicting Reddit post virality. It includes:\n",
    "\n",
    "1. **Data Loading and Exploration**: Loading and understanding the raw Reddit dataset from *reddit_raw_data*\n",
    "2. **Feature Engineering**: Creating features from raw data (text features, engagement metrics, subreddit features)\n",
    "3. **Virality Score Computation**: Computing virality scores and creating binary labels\n",
    "\n",
    "The processed data will be saved to `data/reddit_features.csv` for use in model construction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy import stats\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in dataset:\n",
      "  - reddit_data_counts.json (0.00 MB)\n",
      "  - reddit_dataset.json (11.27 MB)\n"
     ]
    }
   ],
   "source": [
    "# Set dataset path to reddit_raw_data folder\n",
    "dataset_path = Path(\"reddit_raw_data\")\n",
    "print(\"Files in dataset:\")\n",
    "for file in dataset_path.iterdir():\n",
    "    print(f\"  - {file.name} ({file.stat().st_size / (1024*1024):.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: reddit_dataset.json\n",
      "File size: 11.27 MB\n",
      "\n",
      "Top-level keys: ['posts']\n",
      "\n",
      "Found 'posts' array with 6187 items\n",
      "\n",
      "Sample post structure:\n",
      "{\n",
      "  \"title\": \"Which country in the world suffers most from wage inequality and why?\",\n",
      "  \"body\": \"Shall we discuss this topic in the comments? I'm curious to hear your opinions. I have written my own thoughts below.\\r  \\n\\r  \\nMany sources and studies highlight countries like Brazil, South Africa, India, and the United States as standing out in terms of income inequality. Inequality factors in these countries can include high income inequality, challenging working conditions faced by low-wage workers, racial or ethnic discrimination, gender inequality, and social class disparities.\\r  \\n\\r  \\nThe causes of income inequality in these countries can be complex and multifaceted. For example, high income inequality can sometimes reflect a wide economic gap between the rich and the poor. Challenging working conditions experienced by low-wage workers can arise from factors such as low job security, low wages, and limited social safety nets. One of the major causes, of course, is poor governanc\n",
      "\n",
      "Keys in post: ['title', 'body', 'url', 'post_score', 'comment', 'comment_score']\n"
     ]
    }
   ],
   "source": [
    "# Explore JSON structure from reddit_dataset.json\n",
    "json_file = dataset_path / \"reddit_dataset.json\"\n",
    "\n",
    "if json_file.exists():\n",
    "    print(f\"Loading: {json_file.name}\")\n",
    "    print(f\"File size: {json_file.stat().st_size / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    # Load and explore structure\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        print(\"\\nTop-level keys:\", list(data.keys()))\n",
    "        \n",
    "        # Check if it has a \"posts\" key\n",
    "        if 'posts' in data:\n",
    "            print(f\"\\nFound 'posts' array with {len(data['posts'])} items\")\n",
    "            if len(data['posts']) > 0:\n",
    "                sample_post = data['posts'][0]\n",
    "                print(\"\\nSample post structure:\")\n",
    "                print(json.dumps(sample_post, indent=2)[:1000])  # First 1000 chars\n",
    "                print(\"\\nKeys in post:\", list(sample_post.keys()))\n",
    "        else:\n",
    "            print(\"\\nFull data structure (first 1000 chars):\")\n",
    "            print(json.dumps(data, indent=2)[:1000])\n",
    "else:\n",
    "    print(f\"File not found: {json_file}\")\n",
    "    print(\"Available files:\")\n",
    "    for file in dataset_path.iterdir():\n",
    "        print(f\"  - {file.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reddit_dataset.json...\n",
      "Loaded 6187 posts from 'posts' array\n",
      "\n",
      "Total records loaded: 6187\n"
     ]
    }
   ],
   "source": [
    "# Load JSON data from reddit_dataset.json\n",
    "json_file = dataset_path / \"reddit_dataset.json\"\n",
    "\n",
    "print(f\"Loading {json_file.name}...\")\n",
    "with open(json_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "    # Extract posts array\n",
    "    if 'posts' in data:\n",
    "        raw_data = data['posts']\n",
    "        print(f\"Loaded {len(raw_data)} posts from 'posts' array\")\n",
    "    elif isinstance(data, list):\n",
    "        raw_data = data\n",
    "        print(f\"Loaded {len(raw_data)} items from JSON array\")\n",
    "    else:\n",
    "        # If it's a single object, wrap it in a list\n",
    "        raw_data = [data]\n",
    "        print(f\"Loaded 1 item from JSON object\")\n",
    "\n",
    "print(f\"\\nTotal records loaded: {len(raw_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (6187, 6)\n",
      "\n",
      "Columns: ['title', 'body', 'url', 'post_score', 'comment', 'comment_score']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>url</th>\n",
       "      <th>post_score</th>\n",
       "      <th>comment</th>\n",
       "      <th>comment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which country in the world suffers most from w...</td>\n",
       "      <td>Shall we discuss this topic in the comments? I...</td>\n",
       "      <td>https://www.reddit.com/r/business/comments/14e...</td>\n",
       "      <td>3</td>\n",
       "      <td>Close your eyes, and you can choose one of the...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Passion</td>\n",
       "      <td>Does your work drive you? Or is it something y...</td>\n",
       "      <td>https://www.reddit.com/r/business/comments/14e...</td>\n",
       "      <td>1</td>\n",
       "      <td>Wow, you and I are the same person. Haha, exce...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Biz Savings Interest Rates</td>\n",
       "      <td>I‚Äôm assuming the answer is obviously that the ...</td>\n",
       "      <td>https://www.reddit.com/r/business/comments/14e...</td>\n",
       "      <td>2</td>\n",
       "      <td>I think your assumption is correct, businesses...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How much is international ocean freight?</td>\n",
       "      <td></td>\n",
       "      <td>https://www.reddit.com/r/business/comments/14e...</td>\n",
       "      <td>1</td>\n",
       "      <td>Way too vague to be answered.\\nFrom where to w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hello everyone I want to start a low budget bu...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.reddit.com/r/business/comments/14e...</td>\n",
       "      <td>1</td>\n",
       "      <td>Thanks üôè</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Which country in the world suffers most from w...   \n",
       "1                                            Passion   \n",
       "2                         Biz Savings Interest Rates   \n",
       "3           How much is international ocean freight?   \n",
       "4  Hello everyone I want to start a low budget bu...   \n",
       "\n",
       "                                                body  \\\n",
       "0  Shall we discuss this topic in the comments? I...   \n",
       "1  Does your work drive you? Or is it something y...   \n",
       "2  I‚Äôm assuming the answer is obviously that the ...   \n",
       "3                                                      \n",
       "4                                                      \n",
       "\n",
       "                                                 url  post_score  \\\n",
       "0  https://www.reddit.com/r/business/comments/14e...           3   \n",
       "1  https://www.reddit.com/r/business/comments/14e...           1   \n",
       "2  https://www.reddit.com/r/business/comments/14e...           2   \n",
       "3  https://www.reddit.com/r/business/comments/14e...           1   \n",
       "4  https://www.reddit.com/r/business/comments/14e...           1   \n",
       "\n",
       "                                             comment  comment_score  \n",
       "0  Close your eyes, and you can choose one of the...              5  \n",
       "1  Wow, you and I are the same person. Haha, exce...              1  \n",
       "2  I think your assumption is correct, businesses...              1  \n",
       "3  Way too vague to be answered.\\nFrom where to w...              1  \n",
       "4                                           Thanks üôè              2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to DataFrame and explore\n",
    "df = pd.DataFrame(raw_data)\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types:\n",
      "title            object\n",
      "body             object\n",
      "url              object\n",
      "post_score        int64\n",
      "comment          object\n",
      "comment_score     int64\n",
      "dtype: object\n",
      "\n",
      "Basic statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_score</th>\n",
       "      <th>comment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6187.000000</td>\n",
       "      <td>6187.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>517.082916</td>\n",
       "      <td>191.261516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1970.486668</td>\n",
       "      <td>1102.608781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1547.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>315.000000</td>\n",
       "      <td>106.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>42256.000000</td>\n",
       "      <td>26998.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         post_score  comment_score\n",
       "count   6187.000000    6187.000000\n",
       "mean     517.082916     191.261516\n",
       "std     1970.486668    1102.608781\n",
       "min        0.000000   -1547.000000\n",
       "25%        1.000000       4.000000\n",
       "50%       13.000000      16.000000\n",
       "75%      315.000000     106.000000\n",
       "max    42256.000000   26998.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data types and basic stats\n",
    "print(\"Data types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nBasic statistics:\")\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "post_score:\n",
      "  Q1: 1.00, Q3: 315.00, IQR: 314.00\n",
      "  Outlier bounds: [-470.00, 786.00]\n",
      "  Number of outliers: 702 (11.35%)\n",
      "  Min: 0, Max: 42256\n",
      "\n",
      "comment_score:\n",
      "  Q1: 4.00, Q3: 106.00, IQR: 102.00\n",
      "  Outlier bounds: [-149.00, 259.00]\n",
      "  Number of outliers: 692 (11.18%)\n",
      "  Min: -1547, Max: 26998\n"
     ]
    }
   ],
   "source": [
    "# Check for outliers\n",
    "numeric_cols = ['post_score', 'comment_score']\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}\")\n",
    "        print(f\"  Outlier bounds: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "        print(f\"  Number of outliers: {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)\")\n",
    "        print(f\"  Min: {df[col].min()}, Max: {df[col].max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded subreddit frequencies from reddit_data_counts.json\n",
      "Original columns: 6\n",
      "Features after engineering: 25\n",
      "\n",
      "New features created:\n",
      "['subreddit', 'combined_text', 'text_length', 'word_count', 'title_length', 'title_word_count', 'body_length', 'body_word_count', 'has_question_mark', 'has_exclamation', 'uppercase_ratio', 'score', 'comment_to_score_ratio', 'total_engagement', 'score_z', 'comment_score_z', 'has_comment', 'comment_length', 'subreddit_frequency']\n"
     ]
    }
   ],
   "source": [
    "# Create features from raw data for Random Forest\n",
    "def extract_features(df):\n",
    "    features_df = df.copy()\n",
    "    \n",
    "    # 1. Extract subreddit from URL\n",
    "    if 'url' in features_df.columns:\n",
    "        features_df['subreddit'] = features_df['url'].astype(str).str.extract(r'/r/([^/]+)/')\n",
    "    \n",
    "    # 2. Text-based features (combine title and body)\n",
    "    if 'title' in features_df.columns and 'body' in features_df.columns:\n",
    "        features_df['combined_text'] = (\n",
    "            features_df['title'].astype(str) + ' ' + features_df['body'].astype(str)\n",
    "        )\n",
    "        text_col = 'combined_text'\n",
    "    elif 'title' in features_df.columns:\n",
    "        text_col = 'title'\n",
    "        features_df['combined_text'] = features_df['title'].astype(str)\n",
    "    elif 'body' in features_df.columns:\n",
    "        text_col = 'body'\n",
    "        features_df['combined_text'] = features_df['body'].astype(str)\n",
    "    else:\n",
    "        text_col = None\n",
    "    \n",
    "    if text_col:\n",
    "        # Character and word counts\n",
    "        features_df['text_length'] = features_df['combined_text'].str.len()\n",
    "        features_df['word_count'] = features_df['combined_text'].str.split().str.len()\n",
    "        \n",
    "        # Title-specific features\n",
    "        if 'title' in features_df.columns:\n",
    "            features_df['title_length'] = features_df['title'].astype(str).str.len()\n",
    "            features_df['title_word_count'] = features_df['title'].astype(str).str.split().str.len()\n",
    "        \n",
    "        # Body-specific features\n",
    "        if 'body' in features_df.columns:\n",
    "            features_df['body_length'] = features_df['body'].astype(str).str.len()\n",
    "            features_df['body_word_count'] = features_df['body'].astype(str).str.split().str.len()\n",
    "        \n",
    "        # Text patterns (use regex=False to treat ? and ! as literal characters)\n",
    "        features_df['has_question_mark'] = features_df['combined_text'].str.contains('?', regex=False, na=False).astype(int)\n",
    "        features_df['has_exclamation'] = features_df['combined_text'].str.contains('!', regex=False, na=False).astype(int)\n",
    "        features_df['uppercase_ratio'] = features_df['combined_text'].apply(\n",
    "            lambda x: sum(1 for c in str(x) if c.isupper()) / len(str(x)) if len(str(x)) > 0 else 0\n",
    "        )\n",
    "    \n",
    "    # 3. Engagement metrics\n",
    "    if 'post_score' in features_df.columns:\n",
    "        features_df['score'] = pd.to_numeric(features_df['post_score'], errors='coerce').fillna(0)\n",
    "    elif 'score' in features_df.columns:\n",
    "        features_df['score'] = pd.to_numeric(features_df['score'], errors='coerce').fillna(0)\n",
    "    \n",
    "    if 'comment_score' in features_df.columns:\n",
    "        features_df['comment_score'] = pd.to_numeric(features_df['comment_score'], errors='coerce').fillna(0)\n",
    "        features_df['comment_to_score_ratio'] = features_df['comment_score'] / (features_df['score'] + 1)\n",
    "        features_df['total_engagement'] = features_df['score'] + features_df['comment_score']\n",
    "        \n",
    "        # Standardize scores using z-scores for comparable scales\n",
    "        score_z_array = stats.zscore(features_df['score'], nan_policy='omit')\n",
    "        comment_score_z_array = stats.zscore(features_df['comment_score'], nan_policy='omit')\n",
    "        \n",
    "        features_df['score_z'] = pd.Series(score_z_array, index=features_df.index).fillna(0)\n",
    "        features_df['comment_score_z'] = pd.Series(comment_score_z_array, index=features_df.index).fillna(0)\n",
    "        \n",
    "        # Virality_score will be computed later with tunable Œ±\n",
    "        # Formula: v = z_post + Œ± √ó z_comment (using z-scores)\n",
    "    \n",
    "    if 'comment' in features_df.columns:\n",
    "        features_df['has_comment'] = (features_df['comment'].astype(str).str.len() > 0).astype(int)\n",
    "        features_df['comment_length'] = features_df['comment'].astype(str).str.len()\n",
    "    \n",
    "    # 4. Subreddit features\n",
    "    if 'subreddit' in features_df.columns:\n",
    "        # Load subreddit frequency from reddit_data_counts.json\n",
    "        counts_file = dataset_path / \"reddit_data_counts.json\"\n",
    "        if counts_file.exists():\n",
    "            with open(counts_file, 'r', encoding='utf-8') as f:\n",
    "                subreddit_freq_dict = json.load(f)\n",
    "            # Map subreddit frequencies from the JSON file\n",
    "            features_df['subreddit_frequency'] = features_df['subreddit'].map(subreddit_freq_dict).fillna(0)\n",
    "            print(f\"Loaded subreddit frequencies from {counts_file.name}\")\n",
    "        else:\n",
    "            # Fallback: calculate from dataset if JSON file not found\n",
    "            subreddit_counts = features_df['subreddit'].value_counts()\n",
    "            features_df['subreddit_frequency'] = features_df['subreddit'].map(subreddit_counts)\n",
    "            print(\"Warning: reddit_data_counts.json not found. Using calculated frequencies.\")\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "# Apply feature engineering\n",
    "df_features = extract_features(df)\n",
    "print(f\"Original columns: {len(df.columns)}\")\n",
    "print(f\"Features after engineering: {len(df_features.columns)}\")\n",
    "print(f\"\\nNew features created:\")\n",
    "new_cols = [col for col in df_features.columns if col not in df.columns]\n",
    "print(new_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before handling:\n",
      "subreddit    1258\n",
      "dtype: int64\n",
      "\n",
      "Missing values after handling:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values\n",
    "missing_before = df_features.isnull().sum()\n",
    "print(\"Missing values before handling:\")\n",
    "print(missing_before[missing_before > 0])\n",
    "\n",
    "# Fill missing values\n",
    "df_features = df_features.fillna(0)\n",
    "\n",
    "missing_after = df_features.isnull().sum()\n",
    "print(\"\\nMissing values after handling:\")\n",
    "print(missing_after[missing_after > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Virality Score Computation and Label Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virality score formula: v = z_post + Œ± √ó z_comment\n",
      "Using default Œ± = 0.5 (will be tuned via grid search)\n",
      "\n",
      "Virality score statistics:\n",
      "count    6187.000000\n",
      "mean        0.000000\n",
      "std         1.388064\n",
      "min        -0.349173\n",
      "25%        -0.346182\n",
      "50%        -0.333980\n",
      "75%        -0.120144\n",
      "max        23.583316\n",
      "Name: virality_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Compute virality score with tunable Œ± (alpha) hyperparameter\n",
    "def compute_virality_score(df, alpha=0.5):\n",
    "    if 'score_z' in df.columns and 'comment_score_z' in df.columns:\n",
    "        return df['score_z'] + alpha * df['comment_score_z']\n",
    "    else:\n",
    "        raise ValueError(\"Missing 'score_z' or 'comment_score_z' columns. Run feature engineering first.\")\n",
    "\n",
    "# Start with default Œ± = 0.5 (will be tuned via grid search)\n",
    "default_alpha = 0.5\n",
    "df_features['virality_score'] = compute_virality_score(df_features, alpha=default_alpha)\n",
    "\n",
    "print(f\"Virality score formula: v = z_post + Œ± √ó z_comment\")\n",
    "print(f\"Using default Œ± = {default_alpha} (will be tuned via grid search)\")\n",
    "print(f\"\\nVirality score statistics:\")\n",
    "print(df_features['virality_score'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Œ± values: [0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0, 1.5]\n",
      "For each Œ±, computing virality score and evaluating model performance\n",
      "\n",
      "Œ± = 0.20: F1 = 0.6291, Precision = 0.6122, Recall = 0.6469, ROC-AUC = 0.8283\n",
      "Œ± = 0.25: F1 = 0.6273, Precision = 0.6240, Recall = 0.6307, ROC-AUC = 0.8305\n",
      "Œ± = 0.30: F1 = 0.6252, Precision = 0.6146, Recall = 0.6361, ROC-AUC = 0.8296\n",
      "Œ± = 0.35: F1 = 0.6146, Precision = 0.6042, Recall = 0.6253, ROC-AUC = 0.8268\n",
      "Œ± = 0.40: F1 = 0.6245, Precision = 0.6108, Recall = 0.6388, ROC-AUC = 0.8285\n",
      "Œ± = 0.45: F1 = 0.6344, Precision = 0.6014, Recall = 0.6712, ROC-AUC = 0.8162\n",
      "Œ± = 0.50: F1 = 0.6192, Precision = 0.5960, Recall = 0.6442, ROC-AUC = 0.8068\n",
      "Œ± = 0.55: F1 = 0.6187, Precision = 0.6230, Recall = 0.6146, ROC-AUC = 0.8118\n",
      "Œ± = 0.60: F1 = 0.6269, Precision = 0.5923, Recall = 0.6658, ROC-AUC = 0.8158\n",
      "Œ± = 0.65: F1 = 0.6121, Precision = 0.6096, Recall = 0.6146, ROC-AUC = 0.8185\n",
      "Œ± = 0.70: F1 = 0.6121, Precision = 0.5995, Recall = 0.6253, ROC-AUC = 0.8209\n",
      "Œ± = 0.75: F1 = 0.6255, Precision = 0.6103, Recall = 0.6415, ROC-AUC = 0.8182\n",
      "Œ± = 0.80: F1 = 0.6019, Precision = 0.5947, Recall = 0.6092, ROC-AUC = 0.8094\n",
      "Œ± = 0.85: F1 = 0.6280, Precision = 0.6150, Recall = 0.6415, ROC-AUC = 0.8174\n",
      "Œ± = 0.90: F1 = 0.6121, Precision = 0.5995, Recall = 0.6253, ROC-AUC = 0.8092\n",
      "Œ± = 0.95: F1 = 0.6094, Precision = 0.6070, Recall = 0.6119, ROC-AUC = 0.8055\n",
      "Œ± = 1.00: F1 = 0.6117, Precision = 0.5779, Recall = 0.6496, ROC-AUC = 0.8109\n",
      "Œ± = 1.50: F1 = 0.6255, Precision = 0.5985, Recall = 0.6550, ROC-AUC = 0.8122\n",
      "  \n",
      "Best Œ± = 0.25\n",
      "  ROC-AUC:   0.8305\n",
      "  F1-Score:  0.6273\n"
     ]
    }
   ],
   "source": [
    "# Grid Search for Optimal Œ± Hyperparameter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score\n",
    "\n",
    "# Note: exclude engagement metrics (score, comment_score, total_engagement, comment_to_score_ratio)\n",
    "# because these are used to compute virality_score, which would cause data leakage\n",
    "base_features = [\n",
    "    'text_length', 'word_count', 'has_question_mark', \n",
    "    'has_exclamation', 'uppercase_ratio',\n",
    "    'title_length', 'title_word_count', 'body_length', 'body_word_count',\n",
    "    'has_comment', 'comment_length',  # Comment presence/length OK, but NOT comment_score\n",
    "    'subreddit_frequency'\n",
    "]\n",
    "X_base = df_features[[col for col in base_features if col in df_features.columns]].fillna(0)\n",
    "\n",
    "# Grid of Œ± values to try\n",
    "alpha_values = [0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0, 1.5]\n",
    "results = []\n",
    "\n",
    "print(f\"\\nTesting Œ± values: {alpha_values}\")\n",
    "print(\"For each Œ±, computing virality score and evaluating model performance\\n\")\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    # Compute virality score with this Œ±\n",
    "    virality_scores = compute_virality_score(df_features, alpha=alpha)\n",
    "    \n",
    "    # Create virality label (top 30%)\n",
    "    threshold = virality_scores.quantile(0.70)\n",
    "    y_temp = (virality_scores >= threshold).astype(int)\n",
    "    \n",
    "    # Split data\n",
    "    X_train_temp, X_test_temp, y_train_temp, y_test_temp = train_test_split(\n",
    "        X_base, y_temp, test_size=0.2, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    rf_temp = RandomForestClassifier(\n",
    "        n_estimators=50,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_temp.fit(X_train_temp, y_train_temp)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_temp = rf_temp.predict(X_test_temp)\n",
    "    y_proba_temp = rf_temp.predict_proba(X_test_temp)[:, 1]\n",
    "    \n",
    "    f1 = f1_score(y_test_temp, y_pred_temp)\n",
    "    auc = roc_auc_score(y_test_temp, y_proba_temp)\n",
    "    precision = precision_score(y_test_temp, y_pred_temp)\n",
    "    recall = recall_score(y_test_temp, y_pred_temp)\n",
    "    \n",
    "    results.append({\n",
    "        'alpha': alpha,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': auc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'threshold': threshold\n",
    "    })\n",
    "    \n",
    "    print(f\"Œ± = {alpha:4.2f}: F1 = {f1:.4f}, Precision = {precision:.4f}, Recall = {recall:.4f}, ROC-AUC = {auc:.4f}\")\n",
    "\n",
    "# Find best Œ±\n",
    "# Use ROC-AUC as primary metric (more robust to class imbalance than F1)\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "best_idx_auc = results_df['roc_auc'].idxmax()\n",
    "best_alpha_auc = results_df.loc[best_idx_auc, 'alpha']\n",
    "best_auc = results_df.loc[best_idx_auc, 'roc_auc']\n",
    "best_f1_auc = results_df.loc[best_idx_auc, 'f1_score']\n",
    "best_precision_auc = results_df.loc[best_idx_auc, 'precision']\n",
    "best_recall_auc = results_df.loc[best_idx_auc, 'recall']\n",
    "\n",
    "# Use ROC-AUC as primary (more robust to class imbalance)\n",
    "best_alpha = best_alpha_auc\n",
    "best_f1 = best_f1_auc\n",
    "best_precision = best_precision_auc\n",
    "best_recall = best_recall_auc\n",
    "\n",
    "print(f\"  \\nBest Œ± = {best_alpha:.2f}\")\n",
    "print(f\"  ROC-AUC:   {best_auc:.4f}\")\n",
    "print(f\"  F1-Score:  {best_f1:.4f}\")\n",
    "\n",
    "# Recompute virality score with best Œ±\n",
    "df_features['virality_score'] = compute_virality_score(df_features, alpha=best_alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virality threshold (top 30%): -0.1655\n",
      "\n",
      "Class distribution:\n",
      "is_viral\n",
      "0    4331\n",
      "1    1856\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create virality label using optimal Œ±\n",
    "if 'virality_score' in df_features.columns:\n",
    "    threshold = df_features['virality_score'].quantile(0.70)\n",
    "    df_features['is_viral'] = (df_features['virality_score'] >= threshold).astype(int)\n",
    "    \n",
    "    print(f\"Virality threshold (top 30%): {threshold:.4f}\")\n",
    "    print(f\"\\nClass distribution:\")\n",
    "    print(df_features['is_viral'].value_counts())\n",
    "else:\n",
    "    print(\"Warning: 'virality_score' column not found. Cannot create virality labels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset shape: (6187, 13)\n",
      "Target: is_viral\n",
      "\n",
      "Feature columns: ['text_length', 'word_count', 'has_question_mark', 'has_exclamation', 'uppercase_ratio', 'title_length', 'title_word_count', 'body_length', 'body_word_count', 'has_comment', 'comment_length', 'subreddit_frequency']\n",
      "\n",
      "Target distribution:\n",
      "is_viral\n",
      "0    4331\n",
      "1    1856\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Select final features for model (remove raw text columns)\n",
    "# The model should predict virality from content/subreddit features, not from engagement metrics\n",
    "feature_columns = [\n",
    "    # Text features\n",
    "    'text_length', 'word_count', 'has_question_mark', \n",
    "    'has_exclamation', 'uppercase_ratio',\n",
    "    'title_length', 'title_word_count', 'body_length', 'body_word_count',\n",
    "    # Comment features (presence/length OK, but NOT comment_score - that's used to define virality)\n",
    "    'has_comment', 'comment_length',\n",
    "    # Subreddit features\n",
    "    'subreddit_frequency',\n",
    "    # Target variable\n",
    "    'is_viral'\n",
    "]\n",
    "\n",
    "# Filter to only columns that exist\n",
    "available_features = [col for col in feature_columns if col in df_features.columns]\n",
    "\n",
    "df_model = df_features[available_features].copy()\n",
    "\n",
    "# Final check for missing values\n",
    "df_model = df_model.fillna(0)\n",
    "\n",
    "print(f\"Final dataset shape: {df_model.shape}\")\n",
    "print(f\"Target: is_viral\")\n",
    "print(f\"\\nFeature columns: {[col for col in df_model.columns if col != 'is_viral']}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df_model['is_viral'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed data to: data/reddit_features.csv\n",
      "Shape: (6187, 13)\n",
      "Columns: ['text_length', 'word_count', 'has_question_mark', 'has_exclamation', 'uppercase_ratio', 'title_length', 'title_word_count', 'body_length', 'body_word_count', 'has_comment', 'comment_length', 'subreddit_frequency', 'is_viral']\n"
     ]
    }
   ],
   "source": [
    "# Save processed data\n",
    "output_dir = Path(\"data\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save processed dataset\n",
    "csv_path = output_dir / \"reddit_features.csv\"\n",
    "df_model.to_csv(csv_path, index=False)\n",
    "print(f\"Saved processed data to: {csv_path}\")\n",
    "print(f\"Shape: {df_model.shape}\")\n",
    "print(f\"Columns: {list(df_model.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs171",
   "language": "python",
   "name": "cs171"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
